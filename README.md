# tabelog 営業リスト作成ツール

## 概要
`tabelog` は、食べログ（https://tabelog.com/）から店舗情報を取得し、営業活動に活用できるリスト（CSV）を作成するための Streamlit アプリです。都道府県とジャンルを指定して検索し、取得したデータを画面表示・ダウンロードできます。

## 主な機能
- 都道府県・ジャンルを指定して食べログをスクレイピング
- ページ範囲の指定（開始ページ・終了ページ）
- 一度に処理できるページ数を 30 ページ未満に制限（メモリ節約・安定化）
- 取得データのテーブル表示（件数表示・概算進捗表示）
- CSV ダウンロード（UTF-8）
- 入力値の保持（URL クエリパラメータに保存され、リロード後も自動復元）
  - prefecture/genre は英字コード（ローマ字）で URL に保存、UI は日本語で表示
  - 例: `?prefecture=tokyo&genre=ramen&start=11&end=20`
- CSV ダウンロード後に自動リロード（セッションをリセットしてタイムアウト回避、URL 経由で状態復元）

## 取得対象と範囲
- 対象 URL 例
  - `https://tabelog.com/{都道府県ローマ字}/rstLst/{ジャンルローマ字}/{page}/`
  - ジャンル未選択時は `https://tabelog.com/{都道府県ローマ字}/rstLst/{page}/`
- ページ範囲: 1〜最大 60 ページ（1ページあたり最大20件想定）

## 取得項目
- 店名
- ジャンル
- 住所（最初の改行まで）
- 電話番号
- 予約・お問い合わせ
- ホームページ
- 席数（最初の p タグのテキスト or 先頭行）

※ 食べログに情報がない項目は空欄になります。

## 動作環境
- Python 3.13
- 主要ライブラリ
  - streamlit
  - requests / beautifulsoup4
  - pandas

依存関係は `pyproject.toml` に定義されています。

## セットアップ
1. 依存関係のインストール（uv を利用）
   ```bash
   uv sync
   ```
2. 仮想環境の有効化
   ```bash
   source .venv/bin/activate
   ```
3. アプリの起動
   ```bash
   streamlit run app.py
   ```

## 使い方（範囲指定・30ページ未満制約）
1. アプリを起動後、サイドバーで以下を設定します。
   - 都道府県（必須・日本語表示、URL では英字コードで保存）
   - ジャンル（任意・日本語表示、URL では英字コードで保存）
   - 開始ページ（1〜60）
   - 終了ページ（1〜60）
2. 「データ取得」ボタンを押します。
3. 入力検証ルール
   - 終了ページは開始ページ以上であること（start <= end）
   - 一度に処理できるページ数は 30 ページ未満（(end - start) < 30）
   - 条件に合わない場合はエラーを表示して処理を中止します。
4. 検証に成功すると、指定範囲（開始〜終了）でスクレイピングを実行し、結果を表示します。CSV ダウンロードも可能です。
5. ダウンロード後はアプリが自動でリロードされます。URL のクエリに現在の選択（prefecture, genre, start, end）が残るため、入力は自動復元されます。
   - 例: `...?prefecture=osaka&genre=izakaya&start=1&end=20`
   - この URL を共有すると、同じ条件で他ユーザーも実行できます。
6. 続けてデータ取得を行う場合、自動でリロードされますが、より確実にメモリを解放するために、手動でブラウザのリロードを行ってください。

ヒント:
- 初回は「開始ページ=1、終了ページ=1」など小さな範囲で動作確認することを推奨します。
- 進捗バーは概算（ページ×20件想定）で表示しています。
- テーブル表示は大量データ時に先頭 1000 行までの表示に制限されます（CSVは全件出力）。
- 60 ページを取得したい場合は、範囲を 30 ページ未満に分割して複数回実行してください（例: 1-29, 30-59 など）。

## 出力
- CSV ファイル（UTF-8, ヘッダ付き, インデックスなし）
- ファイル名例: `tabelog_{prefecture}_{genre}_range_{start}-{end}pages.csv`
  - `{prefecture}` と `{genre}` は URL の英字コードに対応（例: tokyo, ramen）

## 注意事項・コンプライアンス
- 本プロジェクトは学習・研究目的で作成されています。
- スクレイピング対象サイトの robots.txt と利用規約を遵守してください。
- アクセス間隔を十分に取り、過剰な負荷を与えないようにしてください（現在は 1 秒/リクエスト程度の待機を実装）。
- 取得データの利用は自己責任でお願いします。

## ディレクトリ構成
```
.
├─ app.py         # Streamlit アプリ本体
├─ scraper.py     # スクレイピング処理
├─ utils.py       # 都道府県/ジャンルのローマ字変換
├─ definition.md  # 要件定義書
├─ pyproject.toml # 依存関係定義
├─ uv.lock        # 依存のロックファイル
└─ README.md      # このファイル
```

## 開発向けメモ
- URL クエリには英字コード（ローマ字）を保存、UI は日本語表示
  - 都道府県: `utils.PREFECTURE_MAP` / 逆引きで日本語表示に復元
  - ジャンル: `utils.GENRE_MAP` / 逆引きで日本語表示に復元
- 入力検証で (end - start) < 30 を強制。ユーザーが分割して実行する運用でメモリを節約。
- 早期終了: ページ内に「見つかりませんでした」等の文言を検知した場合、最終ページ到達と判断して処理を終了します。
- 安定性向上: 検索結果ページの構造変化に対応するため、「見つかりませんでした」という文言を直接検知して最終ページと判断するロジックを追加。これにより、無関係なデータの混入や、正規データの取得漏れを防ぎます。
- 例外処理: リクエストの HTTP エラーは握りつつエラーログを出力してスキップします。

## トラブルシューティング
- 依存エラーが出る: `uv sync` を再実行。
- ページが取得されない: 指定した都道府県/ジャンルのローマ字変換が正しいか `utils.py` を確認。
- メモリ不足が続く: 範囲をさらに小さく分割して実行してください（例: 10ページずつ）。
- ダウンロード後に入力が消える: 仕様上、DL後はリロードされますが、URLクエリにより入力は復元されます。URLが変更されていないか確認してください。
- **検索結果があるはずなのに「見つかりませんでした」と表示される:** 食べログ側のページ構造が変更された可能性があります。`scraper.py` の `get_page_content` 関数で判定している「結果なし」の文言やセレクタが古い可能性があります。

## ライセンス
MIT License

## 更新履歴
- 2025-09: 最大取得ページ数の上限を 60 → 40 に変更（UI・内部ロジック・ドキュメントを更新）
- 2025-09: 最大取得ページ数の上限を 40 → 60 に変更（max60 ブランチ）
- 2025-09: スクレイピングの段階実行フローを廃止し、ユーザーが開始・終了ページを指定して「一度に 30 ページ未満」で実行する仕様に変更。
- 2025-09: CSV ダウンロード後に自動でリロードし、URL クエリ（英字コード）を使って入力値を復元する仕様を追加。
- 2025-09: 検索結果の最終ページ判定ロジックを改善し、無関係なデータが混入するバグ、および正規のデータが取得できなくなるバグを修正。